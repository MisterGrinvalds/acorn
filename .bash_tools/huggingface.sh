#!/bin/sh
# Hugging Face Local Model Management
# Provides functions for running Hugging Face models locally using transformers

# Setup Hugging Face environment
hf_setup() {
    echo "ü§ó Setting up Hugging Face environment..."
    
    # Check if we're in a virtual environment
    if [ -z "$VIRTUAL_ENV" ]; then
        echo "üì¶ Creating virtual environment for Hugging Face..."
        if command -v mkvenv >/dev/null 2>&1; then
            mkvenv huggingface
            venv huggingface
        else
            python3 -m venv ~/.venvs/huggingface
            source ~/.venvs/huggingface/bin/activate
        fi
    fi
    
    echo "üì¶ Installing Hugging Face packages..."
    pip install --upgrade pip
    pip install transformers torch torchvision torchaudio
    pip install datasets tokenizers accelerate
    pip install huggingface_hub
    
    # Optional: Install optimized packages
    echo "üöÄ Installing performance optimizations..."
    pip install optimum[onnxruntime]
    
    echo "‚úÖ Hugging Face environment setup complete!"
    echo ""
    echo "üöÄ Try these commands:"
    echo "  hf_models                    # List popular models"
    echo "  hf_download microsoft/DialoGPT-small  # Download model"
    echo "  hf_generate 'Hello world'    # Generate text"
    echo "  hf_chat                      # Interactive chat"
}

# List popular models
hf_models() {
    echo "ü§ó Popular Hugging Face Models"
    echo "=============================="
    echo ""
    echo "üìù Text Generation:"
    echo "  microsoft/DialoGPT-small       # Conversational AI (117M)"
    echo "  microsoft/DialoGPT-medium      # Conversational AI (345M)"
    echo "  gpt2                           # GPT-2 (124M)"
    echo "  distilgpt2                     # Distilled GPT-2 (82M)"
    echo "  EleutherAI/gpt-neo-125m        # GPT-Neo (125M)"
    echo ""
    echo "üß† Language Understanding:"
    echo "  distilbert-base-uncased        # Efficient BERT (66M)"
    echo "  bert-base-uncased              # BERT (110M)"
    echo "  roberta-base                   # RoBERTa (125M)"
    echo ""
    echo "üåç Multilingual:"
    echo "  distilbert-base-multilingual-cased  # Multilingual BERT"
    echo "  xlm-roberta-base               # Multilingual RoBERTa"
    echo ""
    echo "üí° Specialized:"
    echo "  microsoft/codebert-base        # Code understanding"
    echo "  facebook/bart-base             # Text summarization"
    echo "  t5-small                       # Text-to-text generation"
    echo ""
    echo "üì• Usage: hf_download <model_name>"
}

# Download and cache a model
hf_download() {
    local model="$1"
    if [ -z "$model" ]; then
        echo "‚ùå Usage: hf_download <model_name>"
        echo "Example: hf_download microsoft/DialoGPT-small"
        return 1
    fi
    
    echo "üì• Downloading model: $model"
    python3 -c "
from transformers import AutoTokenizer, AutoModel
import sys

try:
    print('üì¶ Downloading tokenizer...')
    tokenizer = AutoTokenizer.from_pretrained('$model')
    print('üì¶ Downloading model...')
    model = AutoModel.from_pretrained('$model')
    print('‚úÖ Model downloaded and cached successfully!')
    print(f'üìç Model info: {model.config}')
except Exception as e:
    print(f'‚ùå Error downloading model: {e}')
    sys.exit(1)
"
}

# Generate text with a model
hf_generate() {
    local prompt="$1"
    local model="${2:-microsoft/DialoGPT-small}"
    
    if [ -z "$prompt" ]; then
        echo "‚ùå Usage: hf_generate <prompt> [model_name]"
        echo "Example: hf_generate 'Hello, how are you?' microsoft/DialoGPT-small"
        return 1
    fi
    
    echo "ü§ñ Generating text with $model..."
    echo "üí¨ Prompt: $prompt"
    echo ""
    
    python3 -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import sys

try:
    # Load model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained('$model')
    model = AutoModelForCausalLM.from_pretrained('$model')
    
    # Add pad token if it doesn't exist
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Encode input
    inputs = tokenizer.encode('$prompt', return_tensors='pt')
    
    # Generate response
    with torch.no_grad():
        outputs = model.generate(
            inputs, 
            max_length=inputs.shape[1] + 50,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract just the generated part
    generated = response[len('$prompt'):].strip()
    print('ü§ñ Response:', generated)
    
except Exception as e:
    print(f'‚ùå Error generating text: {e}')
    sys.exit(1)
"
}

# Interactive chat with a model
hf_chat() {
    local model="${1:-microsoft/DialoGPT-small}"
    
    echo "ü§ñ Starting interactive chat with $model"
    echo "Type 'exit' or 'quit' to end the conversation"
    echo "============================================"
    
    python3 -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import sys

try:
    print('üì¶ Loading model and tokenizer...')
    tokenizer = AutoTokenizer.from_pretrained('$model')
    model = AutoModelForCausalLM.from_pretrained('$model')
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print('‚úÖ Model loaded! Ready to chat.')
    print()
    
    chat_history = []
    
    while True:
        try:
            user_input = input('You: ')
            if user_input.lower() in ['exit', 'quit', 'bye']:
                print('üëã Goodbye!')
                break
            
            # Add user input to history
            chat_history.append(user_input)
            
            # Create context from recent history
            context = ' '.join(chat_history[-5:])  # Last 5 exchanges
            
            # Encode and generate
            inputs = tokenizer.encode(context, return_tensors='pt')
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 50,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated = response[len(context):].strip()
            
            print(f'Bot: {generated}')
            chat_history.append(generated)
            
        except KeyboardInterrupt:
            print('\nüëã Chat interrupted. Goodbye!')
            break
        except Exception as e:
            print(f'‚ùå Error: {e}')
            continue

except Exception as e:
    print(f'‚ùå Error loading model: {e}')
    sys.exit(1)
"
}

# Summarize text
hf_summarize() {
    local text="$1"
    local model="${2:-facebook/bart-large-cnn}"
    
    if [ -z "$text" ]; then
        echo "‚ùå Usage: hf_summarize <text> [model_name]"
        echo "Example: hf_summarize 'Long text to summarize...'"
        return 1
    fi
    
    echo "üìÑ Summarizing text with $model..."
    
    python3 -c "
from transformers import pipeline
import sys

try:
    summarizer = pipeline('summarization', model='$model')
    
    text = '''$text'''
    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)
    
    print('üìù Summary:', summary[0]['summary_text'])
    
except Exception as e:
    print(f'‚ùå Error summarizing: {e}')
    sys.exit(1)
"
}

# Sentiment analysis
hf_sentiment() {
    local text="$1"
    local model="${2:-distilbert-base-uncased-finetuned-sst-2-english}"
    
    if [ -z "$text" ]; then
        echo "‚ùå Usage: hf_sentiment <text> [model_name]"
        echo "Example: hf_sentiment 'I love this product!'"
        return 1
    fi
    
    echo "üòä Analyzing sentiment with $model..."
    
    python3 -c "
from transformers import pipeline
import sys

try:
    classifier = pipeline('sentiment-analysis', model='$model')
    
    result = classifier('$text')
    
    print(f'üòä Sentiment: {result[0][\"label\"]} (confidence: {result[0][\"score\"]:.2f})')
    
except Exception as e:
    print(f'‚ùå Error analyzing sentiment: {e}')
    sys.exit(1)
"
}

# Question answering
hf_qa() {
    local question="$1"
    local context="$2"
    local model="${3:-distilbert-base-cased-distilled-squad}"
    
    if [ -z "$question" ] || [ -z "$context" ]; then
        echo "‚ùå Usage: hf_qa <question> <context> [model_name]"
        echo "Example: hf_qa 'What is Python?' 'Python is a programming language...'"
        return 1
    fi
    
    echo "‚ùì Answering question with $model..."
    
    python3 -c "
from transformers import pipeline
import sys

try:
    qa_pipeline = pipeline('question-answering', model='$model')
    
    result = qa_pipeline(question='$question', context='$context')
    
    print(f'‚ùì Question: $question')
    print(f'üìù Answer: {result[\"answer\"]} (confidence: {result[\"score\"]:.2f})')
    
except Exception as e:
    print(f'‚ùå Error answering question: {e}')
    sys.exit(1)
"
}

# Code completion
hf_code() {
    local code_prompt="$1"
    local model="${2:-microsoft/CodeGPT-small-py}"
    
    if [ -z "$code_prompt" ]; then
        echo "‚ùå Usage: hf_code <code_prompt> [model_name]"
        echo "Example: hf_code 'def fibonacci(n):'"
        return 1
    fi
    
    echo "üßë‚Äçüíª Generating code with $model..."
    
    python3 -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import sys

try:
    tokenizer = AutoTokenizer.from_pretrained('$model')
    model = AutoModelForCausalLM.from_pretrained('$model')
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    inputs = tokenizer.encode('$code_prompt', return_tensors='pt')
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=inputs.shape[1] + 100,
            num_return_sequences=1,
            temperature=0.3,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print('üßë‚Äçüíª Generated code:')
    print('```python')
    print(response)
    print('```')
    
except Exception as e:
    print(f'‚ùå Error generating code: {e}')
    # Try alternative approach
    try:
        from transformers import pipeline
        generator = pipeline('text-generation', model='gpt2')
        result = generator('$code_prompt', max_length=150, temperature=0.3)
        print('üßë‚Äçüíª Generated code (alternative):')
        print(result[0]['generated_text'])
    except:
        print('‚ùå Code generation failed')
        sys.exit(1)
"
}

# Check model status and cache
hf_status() {
    echo "ü§ó Hugging Face Status"
    echo "====================="
    
    # Check if transformers is installed
    if python3 -c "import transformers" 2>/dev/null; then
        echo "‚úÖ Transformers installed"
        python3 -c "import transformers; print(f'   Version: {transformers.__version__}')"
    else
        echo "‚ùå Transformers not installed"
        echo "   Run: hf_setup"
        return 1
    fi
    
    # Check if torch is installed
    if python3 -c "import torch" 2>/dev/null; then
        echo "‚úÖ PyTorch installed"
        python3 -c "import torch; print(f'   Version: {torch.__version__}')"
    else
        echo "‚ùå PyTorch not installed"
    fi
    
    # Check cache directory
    echo ""
    echo "üìÅ Model cache:"
    if [ -d ~/.cache/huggingface ]; then
        echo "   Location: ~/.cache/huggingface"
        if command -v du >/dev/null 2>&1; then
            echo "   Size: $(du -sh ~/.cache/huggingface 2>/dev/null | cut -f1)"
        fi
        echo "   Models:"
        find ~/.cache/huggingface -name "config.json" 2>/dev/null | head -5 | while read -r config; do
            model_dir=$(dirname "$config")
            model_name=$(basename "$(dirname "$model_dir")")/$(basename "$model_dir")
            echo "     - $model_name"
        done
    else
        echo "   No cache directory found"
    fi
    
    # Check virtual environment
    echo ""
    if [ -n "$VIRTUAL_ENV" ]; then
        echo "‚úÖ Virtual environment active: $(basename "$VIRTUAL_ENV")"
    else
        echo "‚ö†Ô∏è No virtual environment active"
        echo "   Consider running: hf_setup"
    fi
}

# Clear model cache
hf_clear_cache() {
    echo "üóëÔ∏è Clearing Hugging Face model cache..."
    
    if [ -d ~/.cache/huggingface ]; then
        echo "üìÅ Cache location: ~/.cache/huggingface"
        if command -v du >/dev/null 2>&1; then
            echo "üíæ Current size: $(du -sh ~/.cache/huggingface 2>/dev/null | cut -f1)"
        fi
        
        read -p "Are you sure you want to clear the cache? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            rm -rf ~/.cache/huggingface
            echo "‚úÖ Cache cleared"
        else
            echo "‚ùå Cache clear cancelled"
        fi
    else
        echo "‚ÑπÔ∏è No cache directory found"
    fi
}

# List available pipelines
hf_pipelines() {
    echo "ü§ó Available Hugging Face Pipelines"
    echo "==================================="
    echo ""
    echo "üìù Text Generation:"
    echo "  hf_generate <prompt>           # Generate text"
    echo "  hf_chat [model]                # Interactive chat"
    echo ""
    echo "üìÑ Text Analysis:"
    echo "  hf_summarize <text>            # Summarize text"
    echo "  hf_sentiment <text>            # Sentiment analysis"
    echo "  hf_qa <question> <context>     # Question answering"
    echo ""
    echo "üßë‚Äçüíª Code:"
    echo "  hf_code <code_prompt>          # Code completion"
    echo ""
    echo "üõ†Ô∏è Management:"
    echo "  hf_models                      # List popular models"
    echo "  hf_download <model>            # Download model"
    echo "  hf_status                      # Check installation"
    echo "  hf_clear_cache                 # Clear model cache"
    echo "  hf_setup                       # Setup environment"
}

# Quick example runner
hf_examples() {
    echo "üéØ Hugging Face Examples"
    echo "======================="
    echo ""
    echo "Running quick examples..."
    echo ""
    
    # Check if environment is ready
    if ! python3 -c "import transformers" 2>/dev/null; then
        echo "‚ùå Transformers not available. Run: hf_setup"
        return 1
    fi
    
    echo "1. üòä Sentiment Analysis:"
    hf_sentiment "I love working with AI models!"
    echo ""
    
    echo "2. üìù Text Generation:"
    hf_generate "The future of AI is" "distilgpt2"
    echo ""
    
    echo "3. ‚ùì Question Answering:"
    hf_qa "What is machine learning?" "Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data."
    echo ""
    
    echo "‚úÖ Examples complete!"
}

# Aliases for convenience
alias hf-setup='hf_setup'
alias hf-status='hf_status'
alias hf-models='hf_models'
alias hf-chat='hf_chat'
alias hf-examples='hf_examples'
alias hf-pipelines='hf_pipelines'